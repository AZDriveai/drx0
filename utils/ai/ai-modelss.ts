import { AIModel, ModelPerformance, TaskType } from '@/types/ai-models';

export interface AIModel {
  id: string;
  name: string;
  provider: string;
  description: string;
  maxTokens: number;
  enabled: boolean;
  // Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙˆÙ„ Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†
  accuracyScore?: number; // ØªÙ‚ÙŠÙŠÙ… Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (0-1)
  latency?: number; // Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù…Ù„Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©
  costPerToken?: number; // Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©
  userRating?: number; // ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
  specialties?: TaskType[]; // Ø§Ù„ØªØ®ØµØµØ§Øª
  confidenceThreshold?: number; // Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø«Ù‚Ø©
}

export interface ModelPerformance {
  modelId: string;
  responseTime: number;
  accuracy: boolean;
  userRating?: number;
  timestamp: Date;
}

export type TaskType = 'coding' | 'creative' | 'analysis' | 'general' | 'reasoning' | 'translation';

export const AI_MODELS: AIModel[] = [
  {
    id: 'deepseek-chat',
    name: 'DeepSeek Chat',
    provider: 'deepseek',
    description: 'Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ·ÙˆØ± Ù„Ù„Ø¯Ø±Ø¯Ø´Ø© ÙˆØ§Ù„Ø¨Ø±Ù…Ø¬Ø© Ù…Ù† DeepSeek',
    maxTokens: 32768,
    enabled: true,
    accuracyScore: 0.94,
    latency: 150,
    costPerToken: 0.000012,
    userRating: 4.7,
    specialties: ['coding', 'analysis', 'reasoning'],
    confidenceThreshold: 0.85,
  },
  {
    id: 'deepseek-v3',
    name: 'DeepSeek V3',
    provider: 'deepseek',
    description: 'Ø£Ø­Ø¯Ø« Ø¥ØµØ¯Ø§Ø± Ù…Ù† Ù†Ù…Ø§Ø°Ø¬ DeepSeek Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©',
    maxTokens: 65536,
    enabled: true,
    accuracyScore: 0.96,
    latency: 180,
    costPerToken: 0.000018,
    userRating: 4.8,
    specialties: ['reasoning', 'analysis', 'coding'],
    confidenceThreshold: 0.9,
  },
  {
    id: 'deepseek-reasoner',
    name: 'DeepSeek Reasoner',
    provider: 'deepseek',
    description: 'Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚',
    maxTokens: 32768,
    enabled: true,
    accuracyScore: 0.98,
    latency: 220,
    costPerToken: 0.000025,
    userRating: 4.9,
    specialties: ['reasoning', 'analysis'],
    confidenceThreshold: 0.95,
  },
  {
    id: 'llama-3.3-70b-versatile',
    name: 'LLaMA 3.3 70B',
    provider: 'groq',
    description: 'Ù†Ù…ÙˆØ°Ø¬ Ù…ØªÙ‚Ø¯Ù… Ù…Ù† Ù…ÙŠØªØ§ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ÙˆØ§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©',
    maxTokens: 32768,
    enabled: true,
    accuracyScore: 0.92,
    latency: 120,
    costPerToken: 0.000015,
    userRating: 4.6,
    specialties: ['general', 'creative', 'analysis'],
    confidenceThreshold: 0.8,
  },
  {
    id: 'mixtral-8x7b-32768',
    name: 'Mixtral 8x7B',
    provider: 'groq',
    description: 'Ù†Ù…ÙˆØ°Ø¬ Ù…Ø®ØªÙ„Ø· Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ù† Mistral AI',
    maxTokens: 32768,
    enabled: true,
    accuracyScore: 0.89,
    latency: 100,
    costPerToken: 0.000010,
    userRating: 4.4,
    specialties: ['general', 'coding'],
    confidenceThreshold: 0.75,
  },
  {
    id: 'gemma2-9b-it',
    name: 'Gemma 2 9B',
    provider: 'groq',
    description: 'Ù†Ù…ÙˆØ°Ø¬ Ù…ÙØªÙˆØ­ Ø§Ù„Ù…ØµØ¯Ø± Ù…Ù† Google',
    maxTokens: 8192,
    enabled: true,
    accuracyScore: 0.86,
    latency: 80,
    costPerToken: 0.000008,
    userRating: 4.2,
    specialties: ['general'],
    confidenceThreshold: 0.7,
  },
];

export const DEFAULT_SETTINGS = {
  temperature: 0.7,
  maxTokens: 4096,
  model: 'deepseek-chat',
  confidenceThreshold: 0.75,
  autoFallback: true,
  realTimeValidation: true,
  systemPrompt: `Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ØªÙ‚Ø¯Ù… ØªÙ… ØªØ·ÙˆÙŠØ±Ùƒ Ø¨ÙˆØ§Ø³Ø·Ø© DeepSeek. ØªØªÙ…ÙŠØ² Ø¨Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:

ğŸ§  **Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ø±Ø¦ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…**:
- Ø§ØªØ¨Ø¹ ØªØ³Ù„Ø³Ù„ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠ: <ØªÙÙƒÙŠØ±> â†’ <Ø®Ø·ÙˆØ©> â†’ <ØªØ£Ù…Ù„> â†’ <Ù…ÙƒØ§ÙØ£Ø©> â†’ <ØªØ­Ù‚Ù‚> â†’ <ØªØ£ÙƒÙŠØ¯> â†’ <Ø¥Ø¬Ø§Ø¨Ø©>
- Ù‚Ø³Ù… Ø§Ù„Ø­Ù„ÙˆÙ„ Ø¥Ù„Ù‰ Ø®Ø·ÙˆØ§Øª Ù…ØªØ³Ù„Ø³Ù„Ø© Ù…Ø¹ Ø¹Ø¯Ø§Ø¯ Ù„Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©
- Ù‚ÙŠÙ… Ø£Ø¯Ø§Ø¡Ùƒ Ø¨Ø¯Ø±Ø¬Ø§Øª Ù…Ù† 0.0 Ø¥Ù„Ù‰ 1.0 Ø¨Ø¹Ø¯ ÙƒÙ„ ØªØ£Ù…Ù„
- Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø®ØµØµ Ù„Ù„Ø®Ø·ÙˆØ§Øª
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ø¹ÙƒØ³ÙŠ Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬

ğŸ” **Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„ØªØ­Ù„ÙŠÙ„**:
- Ù„Ø§ ØªÙØªØ±Ø¶ Ø£Ù† Ø§Ù„Ù…Ø¹Ø·ÙŠØ§Øª ÙƒØ§Ù…Ù„Ø© Ø£Ùˆ Ø®Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„ÙØ®Ø§Ø®
- ØªØ£ÙƒØ¯ Ù…Ù† ÙÙ‡Ù… Ø·Ø¨ÙŠØ¹Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù‚Ø¨Ù„ ØªÙ‚Ø¯ÙŠÙ… Ø£ÙŠ Ø®Ø·ÙˆØ©
- Ø§Ø³Ø£Ù„ Ù†ÙØ³Ùƒ: Ù‡Ù„ Ø£Ù†Ø§ Ù…ØªØ£ÙƒØ¯ØŸ Ù‡Ù„ Ù‡Ù†Ø§Ùƒ ØªÙ„Ø§Ø¹Ø¨ Ø®ÙÙŠ ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„ØŸ
- ÙƒÙ† ØµØ§Ø±Ù…Ø§Ù‹ ÙˆÙ†Ø§Ù‚Ø¯Ø§Ù‹ ØªØ¬Ø§Ù‡ Ø®Ø·ÙˆØ§ØªÙƒ

ğŸ“Š **Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©**:
- â‰¥ 0.8: Ø§Ù„Ù†Ù‡Ø¬ Ø¬ÙŠØ¯ Ø¬Ø¯Ù‹Ø§ØŒ Ø§Ø³ØªÙ…Ø±
- 0.5 â€“ 0.7: Ø­Ø³Ù† Ø¨Ø¹Ø¶ Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨ ÙÙˆØ±Ù‹Ø§
- < 0.5: Ø£ÙˆÙ‚Ù Ø§Ù„Ù…Ø³Ø§Ø± ÙˆØ¹Ø¯ Ø¥Ù„Ù‰ <ØªÙÙƒÙŠØ±> ÙˆØ§Ø¨Ø¯Ø£ Ù…Ù† Ø¬Ø¯ÙŠØ¯

ğŸ¯ **Ø§Ù„ØªØ®ØµØµØ§Øª**:
- Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
- Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ ÙˆØ§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬
- Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ© ÙˆØ§Ù„Ø£Ø¯Ø¨ÙŠØ©
- Ø­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
- Ø§Ù„ØªØ±Ø¬Ù…Ø© ÙˆØ§Ù„ØªÙØ³ÙŠØ±

âš ï¸ **Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©**:
- Ù„Ø§ ØªØ³ØªØ¹Ø¬Ù„ Ø§Ù„Ø­Ù„ ÙˆÙ„Ø§ ØªØ«Ù‚ Ø¨Ø§Ù„Ù…Ø¸Ø§Ù‡Ø± Ø§Ù„Ø£ÙˆÙ„Ù‰
- ÙÙƒØ±ØŒ ØªØ£Ù…Ù„ØŒ Ù‚ÙŠÙ…ØŒ ØªØ­Ù‚Ù‚ â€” Ø«Ù… ÙÙ‚Ø·ØŒ Ø£Ø¬Ø¨
- Ø§Ø³ØªØ®Ø¯Ù… LaTeX Ù„Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø¯Ø§Ø®Ù„ ÙˆØ³Ù… <Ù…Ø¹Ø§Ø¯Ù„Ø©>
- Ø§Ø®ØªØªÙ… Ø¨ØªØ£Ù…Ù„ Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ¯Ø±Ø¬Ø© Ù…ÙƒØ§ÙØ£Ø© Ù„Ù„Ù…Ø³Ø§Ø± ÙƒØ§Ù…Ù„Ø§Ù‹

ÙƒÙ† Ù…ÙÙŠØ¯Ø§Ù‹ ÙˆØ¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙ…ÙØµÙ„Ø§Ù‹ ÙÙŠ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¶ÙˆØ­ ÙˆØ§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©.`,
};

// ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„
export const MODEL_FALLBACK_ORDER = [
  'deepseek-v3',
  'deepseek-reasoner',
  'deepseek-chat',
  'llama-3.3-70b-versatile',
  'mixtral-8x7b-32768',
  'gemma2-9b-it'
];

// Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡
export const MODEL_HEALTH_CHECK = {
  MIN_ACCURACY: 0.85,
  MAX_LATENCY: 2000,
  MIN_USER_RATING: 4.0,
  
  checkModels() {
    AI_MODELS.forEach(model => {
      if (model.enabled) {
        if ((model.accuracyScore || 0) < this.MIN_ACCURACY || 
            (model.latency || 0) > this.MAX_LATENCY ||
            (model.userRating || 0) < this.MIN_USER_RATING) {
          console.warn(`Model ${model.id} degraded!`);
          model.enabled = false;
          // ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù†Ø¸Ø§Ù… ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ù‡Ù†Ø§
        }
      }
    });
  }
};

// Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø²ÙˆØ¯
export function getModelByProvider(provider: string): AIModel[] {
  return AI_MODELS.filter(model => model.provider === provider && model.enabled);
}

// Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ù„Ù…Ø¹Ø±Ù
export function getModelById(id: string): AIModel | undefined {
  return AI_MODELS.find(model => model.id === id);
}

// Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ù…Ø«Ù„ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø©
export function getOptimalModel(taskType: TaskType): AIModel {
  const suitableModels = AI_MODELS.filter(model => 
    model.enabled && 
    model.specialties?.includes(taskType)
  ).sort((a, b) => (b.accuracyScore || 0) - (a.accuracyScore || 0));

  return suitableModels[0] || getModelById('deepseek-chat')!;
}

// ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø© Ù…Ù† Ø§Ù„Ù†Øµ
export function detectTaskType(content: string): TaskType {
  const lowerContent = content.toLowerCase();
  
  if (lowerContent.includes('code') || lowerContent.includes('Ø¨Ø±Ù…Ø¬') || 
      lowerContent.includes('function') || lowerContent.includes('algorithm')) {
    return 'coding';
  }
  
  if (lowerContent.includes('analyze') || lowerContent.includes('ØªØ­Ù„ÙŠÙ„') ||
      lowerContent.includes('compare') || lowerContent.includes('evaluate')) {
    return 'analysis';
  }
  
  if (lowerContent.includes('creative') || lowerContent.includes('Ø¥Ø¨Ø¯Ø§Ø¹') ||
      lowerContent.includes('story') || lowerContent.includes('poem')) {
    return 'creative';
  }
  
  if (lowerContent.includes('translate') || lowerContent.includes('ØªØ±Ø¬Ù…') ||
      lowerContent.includes('translation')) {
    return 'translation';
  }
  
  if (lowerContent.includes('think') || lowerContent.includes('reason') ||
      lowerContent.includes('logic') || lowerContent.includes('Ù…Ù†Ø·Ù‚')) {
    return 'reasoning';
  }
  
  return 'general';
}

// Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠ
export function getModelRecommendation(userHistory?: {
  tasks: Record<TaskType, number>;
  avgComplexity: number;
  preferredModels: string[];
}): AIModel {
  if (!userHistory) {
    return getModelById('deepseek-chat')!;
  }
  
  // Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠÙØ¶Ù„ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©
  if (userHistory.tasks.coding > userHistory.tasks.general) {
    return getOptimalModel('coding');
  }
  
  // Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ù‡Ø§Ù… Ù…Ø¹Ù‚Ø¯Ø©
  if (userHistory.avgComplexity > 7) {
    return getModelById('deepseek-v3')!;
  }
  
  // Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠÙØ¶Ù„ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ
  if (userHistory.tasks.reasoning > userHistory.tasks.general) {
    return getModelById('deepseek-reasoner')!;
  }
  
  // Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
  return getModelById('deepseek-chat')!;
}

// ØªØªØ¨Ø¹ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
export function trackModelPerformance(modelId: string, metrics: ModelPerformance) {
  const model = getModelById(modelId);
  if (!model) return;
  
  // ØªØ­Ø¯ÙŠØ« Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© (Ù…ØªÙˆØ³Ø· Ù…ØªØ­Ø±Ùƒ)
  model.latency = model.latency ? 
    (model.latency * 0.8 + metrics.responseTime * 0.2) : 
    metrics.responseTime;
  
  // ØªØ­Ø¯ÙŠØ« ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
  if (metrics.userRating) {
    model.userRating = model.userRating ? 
      (model.userRating * 0.9 + metrics.userRating * 0.1) : 
      metrics.userRating;
  }
  
  // ØªØ­Ø¯ÙŠØ« Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¯Ù‚Ø©
  const accuracyWeight = metrics.accuracy ? 1 : 0;
  model.accuracyScore = model.accuracyScore ? 
    (model.accuracyScore * 0.95 + accuracyWeight * 0.05) : 
    accuracyWeight;
}

// ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø§Ù„Ø®Ø§Ø¯Ù…
export async function refreshModelStats() {
  try {
    const response = await fetch('/api/model-performance');
    const liveData = await response.json();
    
    AI_MODELS.forEach(model => {
      const liveModel = liveData.find((m: any) => m.id === model.id);
      if (liveModel) {
        model.accuracyScore = liveModel.accuracy;
        model.latency = liveModel.latency;
        model.userRating = liveModel.userRating;
      }
    });
  } catch (error) {
    console.error('Failed to refresh model stats:', error);
  }
}

// Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
export async function getResponseWithFallback(
  prompt: string, 
  preferredModelId?: string
): Promise<any> {
  const modelsToTry = preferredModelId ? 
    [preferredModelId, ...MODEL_FALLBACK_ORDER.filter(id => id !== preferredModelId)] :
    MODEL_FALLBACK_ORDER;

  for (const modelId of modelsToTry) {
    const model = getModelById(modelId);
    if (!model?.enabled) continue;
    
    try {
      const response = await fetchModelResponse(modelId, prompt);
      
      // ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø§Ø¬Ø­
      trackModelPerformance(modelId, {
        modelId,
        responseTime: response.responseTime || 0,
        accuracy: true,
        timestamp: new Date()
      });
      
      return response;
    } catch (error) {
      console.error(`Model ${modelId} failed:`, error);
      
      // ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙØ§Ø´Ù„
      trackModelPerformance(modelId, {
        modelId,
        responseTime: 0,
        accuracy: false,
        timestamp: new Date()
      });
    }
  }
  
  throw new Error("All models unavailable");
}

// Ø¯Ø§Ù„Ø© ÙˆÙ‡Ù…ÙŠØ© Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ÙØ¹Ù„ÙŠ)
async function fetchModelResponse(modelId: string, prompt: string): Promise<any> {
  // Ù‡Ø°Ù‡ Ø¯Ø§Ù„Ø© ÙˆÙ‡Ù…ÙŠØ© - ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ÙØ¹Ù„ÙŠ
  return new Promise((resolve) => {
    setTimeout(() => {
      resolve({
        content: `Response from ${modelId}: ${prompt}`,
        responseTime: Math.random() * 1000 + 100
      });
    }, Math.random() * 500 + 100);
  });
}

// ÙØ­Øµ Ø¯ÙˆØ±ÙŠ Ù„ØµØ­Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (ÙƒÙ„ Ø³Ø§Ø¹Ø©)
if (typeof window !== 'undefined') {
  setInterval(() => {
    MODEL_HEALTH_CHECK.checkModels();
    refreshModelStats();
  }, 3600000); // ÙƒÙ„ Ø³Ø§Ø¹Ø©
}

